{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1154,"status":"ok","timestamp":1720192743262,"user":{"displayName":"AltF4 Magic","userId":"09858635424814148986"},"user_tz":-180},"id":"eoR3x0a7nBvM","outputId":"25dd33e8-125c-4956-c2d2-6c025fec7b91"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 59515 entries, 0 to 59514\n","Data columns (total 4 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   ID      59515 non-null  int64 \n"," 1   USER    59515 non-null  object\n"," 2   TYPE    59515 non-null  int64 \n"," 3   TEXT    59515 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 1.8+ MB\n"]},{"output_type":"execute_result","data":{"text/plain":["0        im getting on borderlands and i will murder yo...\n","1        I am coming to the borders and I will kill you...\n","2        im getting on borderlands and i will kill you ...\n","3        im coming on borderlands and i will murder you...\n","4        im getting on borderlands 2 and i will murder ...\n","                               ...                        \n","59510    I have noticed streamers I watch who are now p...\n","59511    @6th__man playing red dead redemption-\\n\\n“Oh ...\n","59512    ♥️ Suikoden 2\\n1️⃣ Alex Kidd in Miracle World\\...\n","59513    Thank you to Matching funds Home Depot RW paym...\n","59514    Late night stream with the boys! Come watch so...\n","Name: TEXT, Length: 59515, dtype: object"]},"metadata":{},"execution_count":3}],"source":[" import pandas as pd\n","\n","df1 = pd.read_csv(filepath_or_buffer = 'twitter_training.csv', names=['ID', 'USER', 'TYPE', 'TEXT'], na_values='Irrelevant')\n","df2 = pd.read_csv(filepath_or_buffer = 'twitter_validation.csv', names=['ID', 'USER', 'TYPE', 'TEXT'], na_values='Irrelevant')\n","df = pd.concat([df1, df2])\n","df.dropna(inplace=True) #delete irrelevant tweets\n","df['TYPE'] = df['TYPE'].map({'Positive': 1, 'Neutral': 0, 'Negative': -1})\n","df.drop_duplicates(inplace=True)\n","df.reset_index(drop = True, inplace = True)\n","df.info()\n","df['TEXT']"]},{"cell_type":"markdown","metadata":{"id":"0F1sX2l2Ty4G"},"source":["#Tweet preprocessor library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueFTcb7oNy69"},"outputs":[],"source":["# !pip install tweet-preprocessor\n","# -> Just deletes emojis -> not good"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4KnBqiMQGxJ"},"outputs":[],"source":["# import preprocessor as p\n","# X = []\n","# for word in df['TEXT']:\n","#   X.append(p.clean(word))"]},{"cell_type":"markdown","metadata":{"id":"YhmdQ94QUo2g"},"source":["#Handmade preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71453,"status":"ok","timestamp":1720192820993,"user":{"displayName":"AltF4 Magic","userId":"09858635424814148986"},"user_tz":-180},"id":"Ebd5_Q9OUwYv","outputId":"a9d09500-db4e-43af-941e-c2c83ea0f22c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n","Installing collected packages: emoji\n","Successfully installed emoji-2.12.1\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["!pip install emoji\n","\n","import emoji\n","from string import punctuation\n","import re\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","from nltk import word_tokenize\n","from nltk.stem import PorterStemmer\n","\n","stemmer = PorterStemmer()\n","text_lst = []\n","for text in df['TEXT']:\n","  newtext = ''\n","  for word in text.split(): #remove mentions and links\n","    if word[0]!='@':\n","      if '.' not in word or '/' not in word:\n","        newtext += word + ' '\n","  newtext = emoji.demojize(newtext) #transform emojis to text\n","  word_list = []\n","  for word in word_tokenize(newtext): #remove punctuation\n","    if word not in punctuation:\n","      word_list.append(word)\n","  stemmed_words = [stemmer.stem(word) for word in word_list]\n","  newtext = ''\n","  for word in stemmed_words:\n","    newtext += re.sub('[^a-zA-Z#]', '', word) + ' '\n","  text_lst.append(newtext)\n","y = df['TYPE']\n","#for i in range(10):\n","#print(X[i], y[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XY1qhFdnF_vB"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(min_df = 5, max_df = .7, stop_words='english', ngram_range=(1,1))\n","X = vectorizer.fit_transform(text_lst)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLq6WbZCDTJO","executionInfo":{"status":"ok","timestamp":1720195412759,"user_tz":-180,"elapsed":1223897,"user":{"displayName":"AltF4 Magic","userId":"09858635424814148986"}},"outputId":"a35797d7-642d-4a9b-e117-c5b75a7401dd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8027388053431908"]},"metadata":{},"execution_count":11}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .8)\n","model = svm.SVC(kernel = 'linear')\n","model.fit(X_train, y_train)\n","model.score(X_test, y_test)"]}],"metadata":{"colab":{"collapsed_sections":["0F1sX2l2Ty4G"],"provenance":[],"authorship_tag":"ABX9TyMzzro/BRFfS5QzY8UDVg7U"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}